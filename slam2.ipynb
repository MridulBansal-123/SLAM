{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd5c703d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-12-03T09:55:48.145188Z",
     "iopub.status.busy": "2025-12-03T09:55:48.144857Z",
     "iopub.status.idle": "2025-12-03T09:55:48.600822Z",
     "shell.execute_reply": "2025-12-03T09:55:48.599409Z"
    },
    "id": "k1UyrPmvbWze",
    "outputId": "dff78b95-4e84-42d3-9921-d0079aad6403",
    "papermill": {
     "duration": 0.461491,
     "end_time": "2025-12-03T09:55:48.602228",
     "exception": false,
     "start_time": "2025-12-03T09:55:48.140737",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authenticating with provided credentials...\n",
      "\n",
      "❌ Error: POST failed with: {\"errors\":[\"New Datasets cannot be attached in non-interactive sessions. Found no versions attached for Dataset [soumikrakshit/nyu-depth-v2].\"],\"error\":{\"code\":9},\"wasSuccessful\":false}\n",
      "Double check that your username and key are pasted correctly inside the quotes above.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import kagglehub\n",
    "\n",
    "# --- CONFIGURATION: PASTE YOUR KAGGLE CREDENTIALS HERE ---\n",
    "# Open your kaggle.json file with a text editor to see these values\n",
    "os.environ['KAGGLE_USERNAME'] = \"meetparmar40\"\n",
    "os.environ['KAGGLE_KEY'] = \"KGAT_c505d522d54ad468e6d6768c30208ffb\"\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "print(\"Authenticating with provided credentials...\")\n",
    "\n",
    "# Download the specific version of the dataset that contains the raw images\n",
    "# This dataset (soumikrakshit/nyu-depth-v2) is the standard repackage of the 50k subset\n",
    "try:\n",
    "    path = kagglehub.dataset_download(\"soumikrakshit/nyu-depth-v2\")\n",
    "    print(f\"\\n✅ Success! Dataset downloaded to: {path}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Error: {e}\")\n",
    "    print(\"Double check that your username and key are pasted correctly inside the quotes above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92526079",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-12-03T09:55:48.607989Z",
     "iopub.status.busy": "2025-12-03T09:55:48.607684Z",
     "iopub.status.idle": "2025-12-03T09:55:48.782935Z",
     "shell.execute_reply": "2025-12-03T09:55:48.781680Z"
    },
    "id": "imoXcKlcfr3n",
    "outputId": "f3f3ba73-c782-415d-e0d9-0490dbdf7544",
    "papermill": {
     "duration": 0.179493,
     "end_time": "2025-12-03T09:55:48.784134",
     "exception": true,
     "start_time": "2025-12-03T09:55:48.604641",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Mounting drive is unsupported in this environment. Use PyDrive2 instead. See examples at https://colab.research.google.com/notebooks/io.ipynb#scrollTo=7taylj9wpsA2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13/1408506528.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    116\u001b[0m   \u001b[0;34m\"\"\"Internal helper to mount Google Drive.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/var/colab/hostname'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m     raise NotImplementedError(\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;34m'Mounting drive is unsupported in this environment. Use PyDrive2'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;34m' instead. See examples at'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Mounting drive is unsupported in this environment. Use PyDrive2 instead. See examples at https://colab.research.google.com/notebooks/io.ipynb#scrollTo=7taylj9wpsA2."
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ea9024",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "abEtOlIPdAPF",
    "outputId": "9dab880e-8505-4b7a-bd95-d255ebde37a9",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "\n",
    "class NYURawDataset(Dataset):\n",
    "    def __init__(self, dataset_root, train=True):\n",
    "        self.train = train\n",
    "\n",
    "        # 1. Locate the CSV file automatically\n",
    "        # We look for 'nyu2_train.csv' anywhere inside the downloaded folder\n",
    "        csv_file = None\n",
    "        for root, dirs, files in os.walk(dataset_root):\n",
    "            if 'nyu2_train.csv' in files:\n",
    "                csv_file = os.path.join(root, 'nyu2_train.csv')\n",
    "                break\n",
    "\n",
    "        if csv_file is None:\n",
    "            raise FileNotFoundError(\"Could not find nyu2_train.csv. Check the dataset path.\")\n",
    "\n",
    "        print(f\"Found Metadata CSV: {csv_file}\")\n",
    "        self.data_map = pd.read_csv(csv_file, header=None)\n",
    "\n",
    "        # 2. AUTO-CALIBRATE PATHS (The Fix)\n",
    "        # The CSV contains paths like: \"/nyu_data/data/nyu2_train/basement_0001a_out/1.jpg\"\n",
    "        # We need to figure out how to map that to your actual local folder.\n",
    "\n",
    "        # Get the first image path from the CSV\n",
    "        sample_path_csv = self.data_map.iloc[0, 0] # e.g., /nyu_data/data/nyu2_train/...\n",
    "\n",
    "        # Determine the file name (e.g., 1.jpg) and its immediate parent folder\n",
    "        # This helps us search for the *actual* location of this specific file\n",
    "        sample_filename = os.path.basename(sample_path_csv)\n",
    "        sample_parent = os.path.basename(os.path.dirname(sample_path_csv))\n",
    "\n",
    "        found_real_path = None\n",
    "        # Walk the directory again to find where this specific sample image really lives\n",
    "        for root, dirs, files in os.walk(dataset_root):\n",
    "            if sample_filename in files and os.path.basename(root) == sample_parent:\n",
    "                found_real_path = os.path.join(root, sample_filename)\n",
    "                break\n",
    "\n",
    "        if found_real_path is None:\n",
    "            raise FileNotFoundError(f\"Could not locate sample image {sample_filename} anywhere in dataset!\")\n",
    "\n",
    "        # 3. Calculate the 'Anchor'\n",
    "        # Now we know where the file *is* (found_real_path) and what the CSV *says* (sample_path_csv).\n",
    "        # We create a logic to merge them.\n",
    "\n",
    "        print(f\"Sample CSV Entry:  {sample_path_csv}\")\n",
    "        print(f\"Actual File Found: {found_real_path}\")\n",
    "\n",
    "        # We will store the root that, when joined with the stripped CSV path, gives the real path.\n",
    "        # Logic: We strip the common suffix from the real path.\n",
    "        # CSV Path usually starts with: /nyu_data/data/nyu2_train/...\n",
    "        # We clean the CSV path to be relative\n",
    "        self.clean_csv_prefix = os.path.dirname(sample_path_csv).lstrip('/\\\\')\n",
    "\n",
    "        # The dataset root for our purposes is the part of the real path *before* the CSV structure starts\n",
    "        # This is a robust way to handle any nesting kagglehub or unzipping adds.\n",
    "        if found_real_path.endswith(sample_filename):\n",
    "             # Remove the filename to get directory\n",
    "             real_dir = os.path.dirname(found_real_path)\n",
    "             # If the CSV path structure matches the tail of the real path, we are good.\n",
    "             if real_dir.endswith(self.clean_csv_prefix):\n",
    "                 start_index = len(real_dir) - len(self.clean_csv_prefix)\n",
    "                 self.img_root = real_dir[:start_index]\n",
    "             else:\n",
    "                 # Fallback: Just use the dataset_root and hope standard join works\n",
    "                 self.img_root = dataset_root\n",
    "\n",
    "        print(f\"Calculated Image Root: {self.img_root}\")\n",
    "\n",
    "        self.normalize = T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_map)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data_map.iloc[idx]\n",
    "        img_rel = row[0].lstrip('/\\\\')\n",
    "        depth_rel = row[1].lstrip('/\\\\')\n",
    "\n",
    "        # Construct path using the calculated root\n",
    "        img_path = os.path.join(self.img_root, img_rel)\n",
    "        depth_path = os.path.join(self.img_root, depth_rel)\n",
    "\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            depth = Image.open(depth_path)\n",
    "        except FileNotFoundError:\n",
    "            # If a specific file is missing, we print a warning and just return a random other one\n",
    "            # to prevent crashing, but we DO NOT recurse endlessly.\n",
    "            # print(f\"Warning: Missing file {img_path}\")\n",
    "            return self.__getitem__(0) # Safety fallback to 0th element\n",
    "\n",
    "        # Preprocessing\n",
    "        transform_resize = T.Resize((224, 224)) # ResNet Standard Size\n",
    "        image = transform_resize(image)\n",
    "        depth = transform_resize(depth)\n",
    "\n",
    "        img_tensor = T.ToTensor()(image)\n",
    "        depth_tensor = T.ToTensor()(depth)\n",
    "\n",
    "        if self.train:\n",
    "            if torch.rand(1) < 0.5:\n",
    "                img_tensor = T.functional.hflip(img_tensor)\n",
    "                depth_tensor = T.functional.hflip(depth_tensor)\n",
    "            if torch.rand(1) < 0.25:\n",
    "                img_tensor = img_tensor[[1, 0, 2], :, :]\n",
    "\n",
    "        img_tensor = self.normalize(img_tensor)\n",
    "        depth_tensor = depth_tensor * 10.0 # Scale to 10m\n",
    "\n",
    "        return img_tensor, depth_tensor\n",
    "\n",
    "# --- RUN DATASET LOADING ---\n",
    "\n",
    "# Ensure path variable exists from your previous kagglehub run\n",
    "if 'path' not in globals():\n",
    "    print(\"Please run the kagglehub download cell first to set 'path'\")\n",
    "else:\n",
    "    try:\n",
    "        # Initialize Dataset\n",
    "        train_dataset = NYURawDataset(dataset_root=path, train=True)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "        # Test valid batch\n",
    "        imgs, depths = next(iter(train_loader))\n",
    "        print(\"\\n✅ SUCCESS! Data loaded correctly.\")\n",
    "        print(f\"Batch Shape: {imgs.shape}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Setup Failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fa69f9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vuuK7EFf5Ptn",
    "outputId": "8d9ae3cd-9162-419b-c37f-f03ae198c223",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Ensure the dataset class from the previous step is loaded\n",
    "if 'train_dataset' not in globals():\n",
    "    raise NameError(\"Please run the 'NYURawDataset' class definition from the previous step first.\")\n",
    "\n",
    "# 1. Split the 50k dataset into Train (80%) and Validation (20%)\n",
    "# The paper used ~50k for training, so we use the majority here.\n",
    "total_size = len(train_dataset)\n",
    "train_size = int(0.8 * total_size)\n",
    "val_size = total_size - train_size\n",
    "\n",
    "print(f\"Total Samples: {total_size}\")\n",
    "print(f\"Splitting into: {train_size} Train, {val_size} Validation\")\n",
    "\n",
    "# Set generator for reproducibility\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "train_set, val_set = random_split(train_dataset, [train_size, val_size], generator=generator)\n",
    "\n",
    "# 2. Create Loaders\n",
    "# We use a batch size of 16 (or 8 if you run out of memory)\n",
    "# Pin_memory=True helps speed up transfer to GPU\n",
    "train_loader = DataLoader(train_set, batch_size=16, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_set, batch_size=16, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(\"✅ DataLoaders created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54360ddb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YFWDw48C39uw",
    "outputId": "141c380f-6c8f-4248-cbe4-e1da2fdb58f0",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title Block 2: Loss Function\n",
    "# 1. Install the missing library first\n",
    "!pip install -q kornia\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import kornia\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, w1=0.1, w2=1.0, w3=0.5):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.w1 = w1 # Depth Pointwise Weight (Paper Eq 4)\n",
    "        self.w2 = w2 # Gradient Weight (Paper Eq 4)\n",
    "        self.w3 = w3 # SSIM Weight (Paper Eq 4)\n",
    "\n",
    "        # Window size 11 is standard for SSIM\n",
    "        self.ssim = kornia.losses.SSIMLoss(window_size=11, reduction='mean')\n",
    "\n",
    "    def gradient_loss(self, pred, target):\n",
    "        # Calculate gradients in X and Y directions (Paper Eq 2)\n",
    "        pred_dx = torch.abs(pred[:, :, :, :-1] - pred[:, :, :, 1:])\n",
    "        pred_dy = torch.abs(pred[:, :, :-1, :] - pred[:, :, 1:, :])\n",
    "        target_dx = torch.abs(target[:, :, :, :-1] - target[:, :, :, 1:])\n",
    "        target_dy = torch.abs(target[:, :, :-1, :] - target[:, :, 1:, :])\n",
    "        return torch.mean(torch.abs(pred_dx - target_dx)) + torch.mean(torch.abs(pred_dy - target_dy))\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        # 1. Pointwise L1 Depth Loss (Paper Eq 1)\n",
    "        l_depth = torch.mean(torch.abs(pred - target))\n",
    "\n",
    "        # 2. Gradient Loss (Paper Eq 2)\n",
    "        l_grad = self.gradient_loss(pred, target)\n",
    "\n",
    "        # 3. SSIM Loss (Paper Eq 3)\n",
    "        # Note: Kornia returns SSIM loss directly (which is usually 1 - SSIM)\n",
    "        l_ssim = self.ssim(pred, target)\n",
    "\n",
    "        # Total Weighted Sum (Paper Eq 4)\n",
    "        return (self.w1 * l_depth) + (self.w2 * l_grad) + (self.w3 * l_ssim)\n",
    "\n",
    "# Initialize Loss and Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = CombinedLoss().to(device)\n",
    "\n",
    "print(f\"✅ Loss function initialized on: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964a4672",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o1XB0CjO4Nql",
    "outputId": "4791c187-e63f-421a-d069-376799659c21",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "class UpSample(nn.Sequential):\n",
    "    def __init__(self, skip_input, output_features):\n",
    "        super(UpSample, self).__init__()\n",
    "        self.convA = nn.Conv2d(skip_input, output_features, kernel_size=3, stride=1, padding=1)\n",
    "        self.leakyreluA = nn.LeakyReLU(0.2)\n",
    "        self.convB = nn.Conv2d(output_features, output_features, kernel_size=3, stride=1, padding=1)\n",
    "        self.leakyreluB = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x, concat_with):\n",
    "        # Upsample the input\n",
    "        up_x = F.interpolate(x, size=[concat_with.size(2), concat_with.size(3)], mode='bilinear', align_corners=True)\n",
    "        # Concatenate with the skip connection from the encoder\n",
    "        return self.leakyreluB(self.convB(self.leakyreluA(self.convA(torch.cat([up_x, concat_with], dim=1)))))\n",
    "\n",
    "class ResNetDepthModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNetDepthModel, self).__init__()\n",
    "\n",
    "        # 1. ENCODER: Pre-trained ResNet-152\n",
    "        # We load the weights to get \"Transfer Learning\" benefits\n",
    "        original_model = models.resnet152(weights=models.ResNet152_Weights.IMAGENET1K_V1)\n",
    "\n",
    "        # Extract layers for Skip Connections\n",
    "        self.encoder_layer0 = nn.Sequential(original_model.conv1, original_model.bn1, original_model.relu)\n",
    "        self.encoder_layer1 = nn.Sequential(original_model.maxpool, original_model.layer1)\n",
    "        self.encoder_layer2 = original_model.layer2\n",
    "        self.encoder_layer3 = original_model.layer3\n",
    "        self.encoder_layer4 = original_model.layer4\n",
    "\n",
    "        # 2. DECODER: CNN with Upsampling\n",
    "        # The channel numbers correspond to ResNet-152 feature map sizes\n",
    "        self.up_block1 = UpSample(skip_input=2048 + 1024, output_features=1024)\n",
    "        self.up_block2 = UpSample(skip_input=1024 + 512, output_features=512)\n",
    "        self.up_block3 = UpSample(skip_input=512 + 256, output_features=256)\n",
    "        self.up_block4 = UpSample(skip_input=256 + 64, output_features=128)\n",
    "\n",
    "        # Final output layer (1 channel for Depth)\n",
    "        self.final_conv = nn.Conv2d(128, 1, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # --- Encoder Pass ---\n",
    "        x0 = self.encoder_layer0(x) # Low level features\n",
    "        x1 = self.encoder_layer1(x0)\n",
    "        x2 = self.encoder_layer2(x1)\n",
    "        x3 = self.encoder_layer3(x2)\n",
    "        x4 = self.encoder_layer4(x3) # High level features (Bottleneck)\n",
    "\n",
    "        # --- Decoder Pass (with Skips) ---\n",
    "        d1 = self.up_block1(x4, x3)\n",
    "        d2 = self.up_block2(d1, x2)\n",
    "        d3 = self.up_block3(d2, x1)\n",
    "        d4 = self.up_block4(d3, x0)\n",
    "\n",
    "        # Final prediction\n",
    "        return torch.sigmoid(self.final_conv(d4)) * 10.0 # Scale 0-1 output to 0-10m\n",
    "\n",
    "# Initialize the correct model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ResNetDepthModel().to(device)\n",
    "\n",
    "print(\"✅ ResNet-152 Depth Model Initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a00e0f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 371
    },
    "id": "5q-mMDyv4PPw",
    "outputId": "04dc9575-cc7a-4b01-e2cf-eb9732611503",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --- IMPORTANT: DEFINE MODEL BEFORE RUNNING THIS ---\n",
    "if 'model' not in globals():\n",
    "    # If you haven't defined the ResNet model yet, run this simple placeholder\n",
    "    # In reality, you should use the ResNet-152 U-Net described in the paper\n",
    "    print(\"⚠️ Warning: Initializing a basic U-Net placeholder. Replace with true ResNet model.\")\n",
    "    model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet',\n",
    "                           in_channels=3, out_channels=1, init_features=32, pretrained=False)\n",
    "\n",
    "model = model.to(device)\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Learning rate from paper or standard default\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "def train_model(model, loader, criterion, optimizer, epochs=1):\n",
    "    model.train()\n",
    "    print(f\"Starting training on {len(loader.dataset)} samples...\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i, (images, depths) in enumerate(loader):\n",
    "            # Move data to GPU\n",
    "            images, depths = images.to(device), depths.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward Pass\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Interpolate: ResNet encoders often output smaller feature maps (e.g. 1/2 size)\n",
    "            # We resize output to match ground truth depth map size\n",
    "            if outputs.shape[-2:] != depths.shape[-2:]:\n",
    "                outputs = F.interpolate(outputs, size=depths.shape[-2:], mode='bilinear', align_corners=True)\n",
    "\n",
    "            # Calculate Loss\n",
    "            loss = criterion(outputs, depths)\n",
    "\n",
    "            # Backward Pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Logging every 100 batches (approx every 1600 images)\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{epochs}], Step [{i}/{len(loader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "        epoch_loss = running_loss / len(loader)\n",
    "        print(f\"Epoch {epoch+1} Complete. Average Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# Run Training\n",
    "# Note: On 50k images, 1 epoch will take significantly longer than before.\n",
    "train_model(model, train_loader, criterion, optimizer, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90efbffc",
   "metadata": {
    "id": "7f_brCJO4Q6Y",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title Upload & Visualize Your Own Image (Corrected)\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms # <--- This fixes your error\n",
    "import matplotlib.pyplot as plt\n",
    "from google.colab import files\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "# Ensure device is defined (in case it wasn't earlier)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 1. Upload Image\n",
    "uploaded = files.upload()\n",
    "filename = next(iter(uploaded))\n",
    "\n",
    "# 2. Preprocess Image\n",
    "input_image = Image.open(io.BytesIO(uploaded[filename])).convert('RGB')\n",
    "original_size = input_image.size\n",
    "input_image_resized = input_image.resize((640, 480)) # Resize to standard NYU size\n",
    "\n",
    "# Convert to Tensor & Normalize\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "input_tensor = transform(input_image_resized).unsqueeze(0).to(device)\n",
    "\n",
    "# 3. Predict\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred = model(input_tensor)\n",
    "    # Resize prediction back to original image size for quality\n",
    "    pred = F.interpolate(pred, size=(original_size[1], original_size[0]), mode='bilinear', align_corners=True)\n",
    "\n",
    "# 4. Visualize\n",
    "depth_map = pred.squeeze().cpu().numpy()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Your Image\")\n",
    "plt.imshow(input_image)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Predicted Depth Map\")\n",
    "# plt.imshow(depth_map, cmap='magma')\n",
    "plt.imshow(depth_map, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4ac8a3",
   "metadata": {
    "id": "luwSHvNE-QjE",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title Visualize in Black & White (Like the Paper)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ... (Assuming you ran the previous prediction step) ...\n",
    "\n",
    "depth_map = pred.squeeze().cpu().numpy()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Your Image\")\n",
    "plt.imshow(input_image)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Predicted Depth Map (Grayscale)\")\n",
    "\n",
    "# CHANGE HERE: Use 'gray' instead of 'magma'\n",
    "# 'gray_r' reverses it (White=Close, Black=Far) which often looks better\n",
    "plt.imshow(depth_map, cmap='gray')\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c3262a",
   "metadata": {
    "id": "Ym8NBfV3_wFi",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5.943276,
   "end_time": "2025-12-03T09:55:49.204485",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-03T09:55:43.261209",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
